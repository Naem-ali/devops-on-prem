serviceMonitors:
- name: huggingface-app
  selector:
    matchLabels:
      app: huggingface-app
  endpoints:
  - port: metrics

prometheus:
  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector: {}
    serviceMonitorNamespaceSelector: {}
    podMonitorSelectorNilUsesHelmValues: false
    podMonitorSelector: {}
    podMonitorNamespaceSelector: {}

    resources:
      requests:
        memory: 256Mi
        cpu: 100m
      limits:
        memory: 512Mi
        cpu: 500m

    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: local-path
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 50Gi

    retentionSize: 45GB
    retentionPeriod: 15d

alertmanager:
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: local-path
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 10Gi

additionalServiceMonitors:
- name: huggingface-frontend
  namespaceSelector:
    matchNames:
    - frontend
  selector:
    matchLabels:
      app.kubernetes.io/name: huggingface-app
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics

- name: k3s-monitoring
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      k8s-app: metrics-server
  endpoints:
  - port: metrics
    interval: 30s

- name: crio-monitoring
  namespaceSelector:
    matchNames:
    - kube-system
  selector:
    matchLabels:
      k8s-app: crio
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

- name: node-exporter
  selector:
    matchLabels:
      app.kubernetes.io/name: node-exporter
  endpoints:
  - port: metrics
    interval: 30s
    scrapeTimeout: 15s
    relabelings:
    - sourceLabels: [ __meta_kubernetes_pod_node_name ]
      targetLabel: node

- name: kube-state-metrics
  selector:
    matchLabels:
      app.kubernetes.io/name: kube-state-metrics
  endpoints:
  - port: http
    interval: 30s
    scrapeTimeout: 15s
    metricRelabelings:
    - sourceLabels: [ pod ]
      targetLabel: pod_name

- name: vault-monitoring
  namespaceSelector:
    matchNames:
    - vault
  selector:
    matchLabels:
      app.kubernetes.io/name: vault
  endpoints:
  - port: metrics
    interval: 30s
    path: /v1/sys/metrics

- name: harbor-monitoring
  namespaceSelector:
    matchNames:
    - infrastructure
  selector:
    matchLabels:
      app: harbor
  endpoints:
  - port: metrics
    interval: 30s

- name: minio-monitoring
  namespaceSelector:
    matchNames:
    - infrastructure
  selector:
    matchLabels:
      app.kubernetes.io/name: minio
  endpoints:
  - port: metrics
    interval: 30s

additionalPrometheusRules:
- name: node.rules
  groups:
  - name: node-alerts
    rules:
    - alert: HighNodeCPU
      expr: instance:node_cpu_utilisation:rate5m > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        description: "CPU usage on {{ $labels.instance }} is above 80%"
    - alert: HighNodeMemory
      expr: instance:node_memory_utilisation:ratio > 0.85
      for: 5m
      labels:
        severity: warning
      annotations:
        description: "Memory usage on {{ $labels.instance }} is above 85%"

- name: node-recording.rules
  groups:
  - name: node-recording
    rules:
    - record: instance:node_cpu_utilisation:rate5m
      expr: 1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))
    - record: instance:node_memory_utilisation:ratio
      expr: 1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes

- name: resource-monitoring.rules
  groups:
  - name: node-resources
    rules:
    - alert: HighCPUUsage
      expr: instance:node_cpu_utilisation:rate5m > 0.85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: High CPU usage on {{ $labels.instance }}
        description: "CPU usage is above 85% for 5 minutes"

    - alert: CriticalCPUUsage
      expr: instance:node_cpu_utilisation:rate5m > 0.95
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: Critical CPU usage on {{ $labels.instance }}
        description: "CPU usage is above 95% for 5 minutes"

    - alert: HighMemoryUsage
      expr: instance:node_memory_utilisation:ratio > 0.85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: High memory usage on {{ $labels.instance }}
        description: "Memory usage is above 85% for 5 minutes"

    - alert: DiskSpaceFilling
      expr: predict_linear(node_filesystem_free_bytes[6h], 24 * 3600) < 0
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: Disk space filling up on {{ $labels.instance }}
        description: "Disk will fill up in 24 hours at current rate"

  - name: kubernetes-resources
    rules:
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 0
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping
        description: "Pod has restarted {{ $value }} times in 15 minutes"

    - alert: PodNotReady
      expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown"}) > 0
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} not ready
        description: "Pod has been in non-ready state for 15 minutes"

    - alert: ContainerOOMKilled
      expr: kube_pod_container_status_terminated_reason{reason="OOMKilled"} == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} OOMKilled
        description: "Container has been killed due to memory pressure"

  - name: storage-alerts
    rules:
    - alert: PersistentVolumeUsageHigh
      expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: PV {{ $labels.persistentvolumeclaim }} usage high
        description: "PV is using more than 85% of its capacity"

- name: certificate-monitoring.rules
  groups:
  - name: certificates
    rules:
    - alert: CertificateExpirationCritical
      expr: |
        avg_over_time(cert_manager_certificate_expiration_timestamp_seconds[5m]) - time() < (72 * 3600)
      for: 1h
      labels:
        severity: critical
        team: security
      annotations:
        summary: Certificate expires in less than 72 hours
        description: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} will expire in less than 72 hours"

    - alert: CertificateRotationFailed
      expr: |
        increase(cert_manager_certificate_renewal_errors_total[1h]) > 0
      for: 15m
      labels:
        severity: critical
        team: security
      annotations:
        summary: Certificate rotation failed
        description: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} failed to rotate"

    - alert: CertificateIssuanceDuration
      expr: |
        rate(cert_manager_certificate_renewal_duration_seconds_sum[5m]) / rate(cert_manager_certificate_renewal_duration_seconds_count[5m]) > 300
      for: 15m
      labels:
        severity: warning
        team: security
      annotations:
        summary: Certificate issuance taking too long
        description: "Certificate issuance for {{ $labels.name }} taking longer than 5 minutes"
